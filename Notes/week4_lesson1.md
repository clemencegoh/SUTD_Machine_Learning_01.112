# Deep learning

Biologically-inspired multi-layer neural networks
- Input layer
- Hidden layer
    - handles most of the logic
- Output layer

---
slide 13:
- w(24)^3 denotes that it is layer 3, position 2
    - this comes from prev layer pos 4
- a is activation function (output)
- w is weight
- Z is input of second layer
- b is the biased (must be accounted for)

- Z(i) = w(i) * a(i-1) + b(i)
- a(i) = f(z(i))
- z(i) is called the weighted input to neurons in layer i
---

Output layer is denoted by capital L



---



**QUESTION**
- WHY DO WE NEED SLIDE 15,
WEIGHT DECAY FOR POINT LOSS
- 


